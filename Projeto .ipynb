{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dad7f9",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "DataSet: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "The goal of this project is to develop a Machine Learning model capable of predicting if a woman has diabetes, with some parameters, such as glucose in the last blood exam or insulin. This model will be a classifier and it will be tested through some different alternatives to check which one has the best outcome. \n",
    "\n",
    "Dependent variable: Outcome\n",
    "\n",
    "Examples of Independent Variables: Life expectancy and Poverty rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9249b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239e13d",
   "metadata": {},
   "source": [
    "# First look at the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e14f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bbee0",
   "metadata": {},
   "source": [
    "We can learn through the info function that there are no null values in the database, therefore we don't need to address any solution to the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d8c56",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abe6ea",
   "metadata": {},
   "source": [
    "As we can see, the dabatabase don't have null values, but, there are multiple 0s. This would make sense in some cases, such as pregnancies, but in others, for example, inslulin and blood pressure. This situations needs to be addressed. The approach we'll take will be by replacing the mean values of the whole column to the missing value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62eecc1",
   "metadata": {},
   "source": [
    "# Dividing in Train Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557574bb",
   "metadata": {},
   "source": [
    "We will be making the division of the train and test set stratifically. This is due to the fact that the parameter \"Diabetes Pedigree Function\" is believed to have a huge impact on the outcome, therefore, we believe that the train and the test sets should have the same proportion. Also, as we have a small amount of samples (m = 768) this type of separation can be usefull since, if we were to not to use it, the proportion would be unequal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27889b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = diabetes['DiabetesPedigreeFunction'].quantile([0.2,0.4,0.6,0.8]).values\n",
    "\n",
    "def check_pedigree(x):\n",
    "    result = 0\n",
    "    for val in q:\n",
    "        if x > val:\n",
    "            result += 1\n",
    "        else:\n",
    "            break\n",
    "    return result\n",
    "diabetes['pedigree_cat'] = diabetes['DiabetesPedigreeFunction'].apply(check_pedigree)\n",
    "diabetes['pedigree_cat'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14293ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "for train_index, test_index in split.split(diabetes, diabetes['pedigree_cat']):\n",
    "    strat_train_set = diabetes.loc[train_index]\n",
    "    strat_test_set = diabetes.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f47b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(strat_train_set['pedigree_cat'].value_counts(True), '\\n', strat_test_set['pedigree_cat'].value_counts(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd61f0",
   "metadata": {},
   "source": [
    "As we can see, both test and train has the same proportions of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a142ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to drop the created column\n",
    "strat_train_set.drop(['pedigree_cat'], axis=1, inplace=True)\n",
    "strat_test_set.drop(['pedigree_cat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50a849",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7282040",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "corr = diabetes_train.corr()\n",
    "sns.heatmap(corr,annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1363d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "corr = diabetes_train.corr(method='kendall')\n",
    "sns.heatmap(corr,annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489cdab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "corr = diabetes_train.corr(method='spearman')\n",
    "sns.heatmap(corr,annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c91c65",
   "metadata": {},
   "source": [
    "As we can see, the kendall correlation seems to be the one with the higher values for the outcome relation with other variables. This indicates that the relationship between the outcome and other variables may not be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c5be1",
   "metadata": {},
   "source": [
    "We can see that the correlation between age and pregnancies is the highest. Also, the outcome appears to have some kind of relation with glucose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f00ae",
   "metadata": {},
   "source": [
    "# Separating Independent and Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the 0s with Nan so that SimpleImputer can operate\n",
    "strat_train_set = strat_train_set.replace(to_replace={\n",
    "             'BloodPressure':{0:np.nan}, \n",
    "             'Insulin':{0:np.nan},\n",
    "             'SkinThickness':{0:np.nan},\n",
    "             'BMI':{0:np.nan},\n",
    "             'Glucose':{0:np.nan},\n",
    "                 })\n",
    "strat_train_set.dropna(subset=['BMI', 'BloodPressure', 'Glucose'], inplace = True)\n",
    "\n",
    "diabetes_train = strat_train_set.drop('Outcome', axis=1)\n",
    "\n",
    "diabetes_labels = strat_train_set['Outcome'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a223e3",
   "metadata": {},
   "source": [
    "## Replacing Invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "diabetes_in = diabetes_train.copy()\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "imputer.fit(diabetes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imputer.transform(diabetes_in)\n",
    "\n",
    "diabetes_inputed = pd.DataFrame(temp, columns=diabetes_in.columns)\n",
    "\n",
    "diabetes_inputed.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea9702",
   "metadata": {},
   "source": [
    "As we can see, there are no more 0s values, although we have huge peeks in the mean values of some parameters, this makes the database much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(diabetes_inputed)\n",
    "\n",
    "temp = scaler.transform(diabetes_inputed)\n",
    "\n",
    "diabetes_train = pd.DataFrame(temp, columns=diabetes_inputed.columns)\n",
    "diabetes_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a7865",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58355cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Existe aleatoriedade dentro do SGDClassifier, por isso o argumento\n",
    "# random_state=RANDOM_SEED.\n",
    "sgd_clf = SGDClassifier(\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "sgd_clf.fit(diabetes_train, diabetes_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108385b7",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158cf3a",
   "metadata": {},
   "source": [
    "Before we test the performance of the Model \"SGD Classifier\" we should know the proportions of the target variable. The reason for this is because if the data has a proportion of 80% of people who have diabetes and 20% who haven´t, and the Classifier has an accuracy of 80%, the model would be as bad as one who just guess constanly \"user with diabetes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_labels.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe869cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = time.process_time()\n",
    "Binary_score = cross_val_score(\n",
    "               sgd_clf,\n",
    "               diabetes_train,\n",
    "               diabetes_labels,\n",
    "               cv=10,\n",
    "               scoring=\"accuracy\",\n",
    "               n_jobs=-1,\n",
    "               )\n",
    "t2 = time.process_time()\n",
    "\n",
    "print(Binary_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56d133",
   "metadata": {},
   "source": [
    "Here we see that the model had an accuracy around 73%, which may seem ok (but not good enough). But, when it compares the porportion of people who has diabetes in the data, we realize that this model was almost as bad as one who keeps only guessing \"people who don't have diabetes\". Therefore, we need to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e1935",
   "metadata": {},
   "source": [
    "## Hyperparameters adjusting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac37fd",
   "metadata": {},
   "source": [
    "It is possible to further improve the performance of the SGDC Classifier by using GridSearch, which combines thousands of possible parameters of the model and finds the best of them. For that, it is necessary to pass a grid of parameters, it is necessary to know the SGDC classifier parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbd935",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf79d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train across 5 folds, that's a total of (6+4)*5=50 rounds of training.\n",
    "param_grid = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n",
    "    'loss': ['log'], # logistic regression,\n",
    "    'penalty': ['l2'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "sgd_clf_grid = GridSearchCV(\n",
    "    sgd_clf,  # Modelo\n",
    "    param_grid,  # Grid\n",
    "    cv=5,  # Partições de C.V.\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c75d9a",
   "metadata": {},
   "source": [
    "Now, the best SGDC classifier was obtained (that is, the model with the best parameters). With that said, it is necessary to train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf_grid.fit(diabetes_train, diabetes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bae22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a398e",
   "metadata": {},
   "source": [
    "After using Grid Search, it is possible to calculate once again the model accuracy, and a higher performance can be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.process_time()\n",
    "Binary_score_grid = cross_val_score(\n",
    "    sgd_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print(Binary_score_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e384b7",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76782e",
   "metadata": {},
   "source": [
    "Lastly, it is also interesting to analyze the features that impacted the most on the prediction of the model. For that, the graphic below was plotted and Insulin was the feature that had the most importance, followed by Glucose and Blood Pressure, the latter having a negative coefficient value, however the magnitude is what matters, and the negative coefficient only indicates that the dependent variable and independent variable have a strong negative correlation. That makes sense, since people with lower Blood Pressure tend to not have diabetes, it is something more predictable. However, when this value gets higher, more outliers can appear, and it becomes more difficult to predict whether a person will have diabetes or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88386ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz = FeatureImportances(sgd_clf, relative=False, topn=3)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec2726",
   "metadata": {},
   "source": [
    "Also, by plotting the importance of the all features, the difference of importance between the three features discussed previously and the reamining features is quite notable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(sgd_clf, relative=False)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d30f29",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f1ac2",
   "metadata": {},
   "source": [
    "Although the accuracy wasn´t really good, it is important to understand in which cases the model has missed. With the confusion matrix it was learned that there was a lot of false negatives. This is unacceptable in this scneario, since the model should never overlook people with diabetes because it is a health situation and people can be harmed if they are not properly diagnosed. Therefore, the model should focous on having a high Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf_grid, diabetes_train, diabetes_labels, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97247f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(diabetes_labels, y_train_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b1862",
   "metadata": {},
   "source": [
    "### Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f9e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(diabetes_labels, y_train_pred))\n",
    "print(recall_score(diabetes_labels, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c053f",
   "metadata": {},
   "source": [
    "As we can see, the Recall is actually low, and now it is important to adjust this value to make it more accpetable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(\n",
    "    sgd_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=3,\n",
    "    method=\"decision_function\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176af6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(diabetes_labels, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Threshold\", fontsize=16)\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cdbe7",
   "metadata": {},
   "source": [
    "With this graph it is possible to discover a value of Threshold that increases the Recall, which is -0.75. With this, it is possible to adjust the results for the aim of decreasing false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > -0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd941210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {}'.format(precision_score(diabetes_labels, y_train_pred_90)))\n",
    "print('Recall: {}'.format(recall_score(diabetes_labels, y_train_pred_90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b20d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Recall\", fontsize=16)\n",
    "plt.ylabel(\"Precision\", fontsize=16)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff3204",
   "metadata": {},
   "source": [
    "It is possible to see that when the Recall is increased, the Precision has the opposite reaction. In the current scneario, this is not a problem, because the goal is to just increase the Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fef37",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(diabetes_labels, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51423af",
   "metadata": {},
   "source": [
    "For the diabetes problem, the ideal format of this curve is when the area below the curve is high thus making the curve closer to the upper left corner. With the SGD classifier, the curve obtained does not match the ideal shape. This concept is known as ROC curve and what is desired in the present study is a high sensitivity, or true positive rate, meaning a high recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bee34",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "t1 = time.process_time()\n",
    "Rand_Forrest_Score = cross_val_score(\n",
    "    forest_clf,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print(Rand_Forrest_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336659c",
   "metadata": {},
   "source": [
    "Through the cross val score, it seems that this classifier has a better perfomance than the SDG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b2810",
   "metadata": {},
   "source": [
    "## Adjusting Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3e637",
   "metadata": {},
   "source": [
    "Similarly to the SGDC model, here the hyperparameters of the model will be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b427f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "}\n",
    "\n",
    "forest_clf_grid = GridSearchCV(\n",
    "    forest_clf,  # Modelo\n",
    "    param_grid,  # Grid\n",
    "    cv=5,  # Partições de C.V.\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc81e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf_grid.fit(diabetes_train, diabetes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7075dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a1600",
   "metadata": {},
   "source": [
    "Now, with the hyperparameters adjusted, there is a slight improvement to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83c521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.process_time()\n",
    "Random_Forrest_grid_Score = cross_val_score(\n",
    "    forest_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print(Random_Forrest_grid_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54738e",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f97406",
   "metadata": {},
   "source": [
    "By plotting the features importance graphs, there are some noticable differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(forest_clf, relative=False, topn=3)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a062d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz = FeatureImportances(forest_clf, relative=False)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e34dad",
   "metadata": {},
   "source": [
    "- Although Glucose still is one of the most important features, now the 3rd best is no longer Blood pressure, it is BMI. In fact, Blood Pressure is the least important feature, indicating the difference of impact in comparison to the previous model.\n",
    "\n",
    "- Blood Pressure now doesn't have a negative coefficent, which means that for the Random Forest higher values are more important to the prediction than lower values, contrary to what happened in SGDC Classifier.\n",
    "\n",
    "- Unlike previsously, now age has a bigger impact on the prediction. This corroborates what the hypothesis made in the exploratory analysis, that age was important to determine if a person will have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O \"score\" vai ser a probabilidade de que a amostra seja da classe positiva.\n",
    "y_probas_forest = cross_val_predict(\n",
    "    forest_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=3,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Gambiarra para desviar do bug #9589 introduzido no Scikit-Learn 0.19.0:\n",
    "y_scores_forest = y_probas_forest[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb9b62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(\n",
    "    diabetes_labels,\n",
    "    y_scores_forest,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"r:\", linewidth=2, label=\"SGD\")\n",
    "plt.plot(fpr_forest, tpr_forest, linewidth=2, label=\"Random Forest\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53fe91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('SGD: {:4f}'.format(roc_auc_score(diabetes_labels, y_scores)))\n",
    "print('RandomForest: {:4f}'.format(roc_auc_score(diabetes_labels, y_scores_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2856ba",
   "metadata": {},
   "source": [
    "As it can be seen, the Random Forest Classifier has a better performance because the area under the curve is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0563c",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f48e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "clf_log = LogisticRegression(random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "t1 = time.process_time()\n",
    "Log_Score = cross_val_score(\n",
    "    clf_log,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print(Log_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47ed6a",
   "metadata": {},
   "source": [
    "## Adjusting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2208cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['liblinear']\n",
    "    },\n",
    "]\n",
    "\n",
    "logistic_clf_grid = GridSearchCV(\n",
    "    clf_log,  # Modelo\n",
    "    param_grid,  # Grid\n",
    "    cv=5,  # Partições de C.V.\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf_grid.fit(diabetes_train, diabetes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bcd7e4",
   "metadata": {},
   "source": [
    "## Features Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d2e45",
   "metadata": {},
   "source": [
    "After analysing the importance of features in the logistic regression model, it is noticable that glucose still remains as the most important feature, and pregnancies also becomes the 2nd most important (like in the SGDC classifier). BMI, like the Random Forest model, also becomes one of the top three most important features. The higher the parameter BMI is, the more likely a person is to develop diabtes, according to a study (https://vitagene.com/blog/does-obesity-cause-type-2-diabetes/), therefore it makes sense that this would be one of the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz = FeatureImportances(clf_log, relative=False, topn=3)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6847163",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(clf_log, relative=False)\n",
    "viz.fit(diabetes_train, diabetes_labels)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41877014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O \"score\" vai ser a probabilidade de que a amostra seja da classe positiva.\n",
    "y_probas_logistic = cross_val_predict(\n",
    "    logistic_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=3,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Gambiarra para desviar do bug #9589 introduzido no Scikit-Learn 0.19.0:\n",
    "y_scores_logistic = y_probas_forest[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1de455",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Score_Grid = cross_val_score(\n",
    "    logistic_clf_grid,\n",
    "    diabetes_train,\n",
    "    diabetes_labels,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "Log_Score_Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cd833",
   "metadata": {},
   "source": [
    "After using the Logistic Regression it is possible to conclude that its performance is better than the SGD classifier, with an accuracy of 76%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072c14e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "clf_svm= SVC(C=1e5, kernel='linear', degree=1,  random_state=RANDOM_SEED)\n",
    "clf_svm.fit(diabetes_train, diabetes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342254df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from pprint import pprint\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    clf_svm,\n",
    "    {\n",
    "        'C': [10.0**k for k in np.arange(-3, 5, 2)], 'kernel': ('linear', 'rbf'),\n",
    "        'degree': [1, 2, 3, 4, 5, 6]\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=100,\n",
    "        test_size=0.33,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "# len(clf_svm.get_params().keys())\n",
    "grid.fit(diabetes_train, diabetes_labels)\n",
    "\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c4580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = np.array([p['C'] for p in grid.cv_results_['params']])\n",
    "M = grid.cv_results_['mean_test_score']\n",
    "S = grid.cv_results_['std_test_score']\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(f'C = {p:.5f}: mean_accuracy = {m:.3f}, stddev_accuracy = {s:.3f}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('Accuracy from CV', fontsize=20)\n",
    "plt.xlabel(r\"$C$\", fontsize=20)\n",
    "plt.ylabel(r\"$accuracy$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General accuracy of SVM\n",
    "M.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "M_set = set(M)\n",
    "M_list = list(M_set)\n",
    "M_list.append(M[random.randint(0,len(M))])\n",
    "M_list.append(M[random.randint(0,len(M))])\n",
    "M_list.append(M[random.randint(0,len(M))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619dc4c3",
   "metadata": {},
   "source": [
    "The idea of SVM algorithm is to trace a curve that can separate when a patient has diabets. However, sometimes the samples are mixed and a curve that completely separates those values is not possible. To calculate the best curve possible, the C parameter used in the algorithm associate penalties to the values that are miscalculated. The higher this parameter is, the less regularization the model will execute, and, hence, overfitting will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd5d4c",
   "metadata": {},
   "source": [
    "This model also has another parameter degree. This determines the degree of the curve that will be used to separate the elements. Whilst creating the model, the grid search was used with this parameter (and also with C), and the outcome was that the model had a better performance when degree was equal to 1. Therefore, the curve obtained is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23037a",
   "metadata": {},
   "source": [
    "Finally, it is discoverded that the general accuracy of the Supporting Vector Machine is the diabetes scenario has a 72%  of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a9d9b",
   "metadata": {},
   "source": [
    "## Features Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "f_importances(clf_svm.coef_[0], diabetes_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5abc8",
   "metadata": {},
   "source": [
    "After analysing the importance of features in the Supporting Vector Machine model, it is noticable that glucose still remains as the most important feature, and pregnancies also becomes the 2nd most important (similary to the previous models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662418f8",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41937",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Logistic Regression': Log_Score_Grid,\n",
    "    'Random Forrest': Random_Forrest_grid_Score,\n",
    "    'Binary Classifier': Binary_score_grid,\n",
    "    'SVM': M_list,\n",
    "}).plot.box(\n",
    "    xlabel='Regressor',\n",
    "    ylabel=r'Accuracy $[\\mathtt{USD}]$',\n",
    "    figsize=(10, 5),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aaf95",
   "metadata": {},
   "source": [
    "Analyzing the general results of accuracy of each model, it is possible to understand that all of them had a similar performance. But analyzing the details it is possible to conclude that Random Forrest had the best general accuracy. Moreover, it is intersting to point out that a specific outlier of Logistic Regression, Random Forrest and Binary Classifier had aproximalary the same accuracy. This may have been due to the fact that all of them had the same cut of the cros val and that section of the database impact the best it could to predict if semeone had or not diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "final_model = forest_clf_grid.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('Outcome', axis=1)\n",
    "y_test = strat_test_set['Outcome'].copy()\n",
    "\n",
    "X_test_prepared = imputer.transform(X_test)\n",
    "X_test_prepared = scaler.transform(X_test_prepared)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "\n",
    "print(f'Accuracy = {final_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca185f",
   "metadata": {},
   "source": [
    "**In Conclusion**, the Random Forrest Classifier perfomed a marvelous job, since it had an accuracy of 70,77% in the test set. This shows that not only it doesn't overfit or underfit but also acts much better then a model which just guess that everybody doesn't have diabetes. This is because in the cell above, it is learned that there were 61,03% of people who didn't have diabetes. Therefore a model that only guess, would score an 61,03% of accuracy and the Random Forrest one was 10% better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cc49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
